{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b769dfea",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### NEEDS pyCloudCompare.py in same directory as this .ipynb file ###\n",
    "\n",
    "### BRIEF OVERVIEW\n",
    "### 1. Takes input of RDLA lichen, turns it into RDLA with volume (represented \n",
    "### as a point cloud), but no meshing via distance mapping\n",
    "### 2. Meshes the RDLA volume using ball pivoting for surface reconstruction\n",
    "### 3. Performs dust accumulation to simulate the use of surface remote sensing\n",
    "### such as laser scanning or photogrammetry on the RDLA mesh\n",
    "### 4. Then samples the lowest z-value points, by placing a grid over the dust accumulation\n",
    "### point cloud and calculating the index of the point in the dust accumulation with the\n",
    "### smallest z-coordinate in each grid space.\n",
    "### 5. Meshes these lowest z coordinate points by calculating normals for each point \n",
    "### and then using Poisson surface reconstruction. \n",
    "### 6. Simplifies this mesh into 1024 faces by using the quadratic edge collapse decimation\n",
    "### algorithm.\n",
    "\n",
    "### HISTOGRAM COMPARISON\n",
    "### 1. Using open3d, calculates the distance between dust accumulation and seed mesh\n",
    "### then samples each band in distances to match the histogram of the laser scan\n",
    "### 2. Then prints the distribution as a histogram to test against the laser scan histogram\n",
    "\n",
    "import os\n",
    "import pymeshlab as ml\n",
    "import numpy as np\n",
    "import open3d as o3d\n",
    "import pyCloudCompare as cc\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import time\n",
    "from time import strftime, localtime\n",
    "import csv\n",
    "from scipy.stats import linregress\n",
    "from pathlib import Path\n",
    "from scipy.stats import wilcoxon\n",
    "from scipy.stats import spearmanr\n",
    "from scipy.stats import chisquare\n",
    "import seaborn as sns\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "mean_dist = []\n",
    "std_dist = []\n",
    "num_points = 0\n",
    "num_points_array = []\n",
    "\n",
    "# Set the input folder containing .obj meshes\n",
    "input_folder = r\"***********************\"\n",
    "# Set the output folder where the processed meshes will be saved\n",
    "output_folder = r\"***********************\"\n",
    "# Set the seed mesh folder for mesh to cloud distance calculation\n",
    "seed_folder = r\"***********************\"\n",
    "\n",
    "#distance map parameters:\n",
    "\n",
    "# grid divison n x n x n\n",
    "distanceMapGridstep = 0.0007\n",
    "# Calculate the clearance value (e.g., 5 mm)\n",
    "clearance = 0.01\n",
    "\n",
    "# Create the output folder if it doesn't exist\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "\n",
    "def import_point_cloud_from_obj(obj_file_path):\n",
    "    try:\n",
    "        # Load the .obj file using open3d\n",
    "        pcd = o3d.io.read_point_cloud(obj_file_path, format='ply')\n",
    "        # Convert the point cloud data to a numpy array\n",
    "        points = pcd.points\n",
    "\n",
    "        # Convert numpy array to a list of tuples (x, y, z)\n",
    "        point_cloud = [tuple(point) for point in points]\n",
    "\n",
    "        return point_cloud\n",
    "        #print(point_cloud)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error importing point cloud:\", e)\n",
    "        return None\n",
    "\n",
    "def divide_point_cloud_into_grid(point_cloud, grid_size=(45, 45)):\n",
    "    # Get the bounding box of the point cloud\n",
    "    min_bound = np.min(point_cloud, axis=0)\n",
    "    max_bound = np.max(point_cloud, axis=0)\n",
    "    \n",
    "    # Calculate the size of each grid cell\n",
    "    grid_size_x = (max_bound[0] - min_bound[0]) / grid_size[0]\n",
    "    grid_size_y = (max_bound[1] - min_bound[1]) / grid_size[1]\n",
    "\n",
    "    # Initialize an array to hold the points in each grid cell\n",
    "    grid_cells = [[[] for _ in range(grid_size[1])] for _ in range(grid_size[0])]\n",
    "    \n",
    "    # Iterate through each point in the point cloud and find its grid cell\n",
    "    for point in point_cloud:\n",
    "        x_idx = int((point[0] - min_bound[0]) // grid_size_x)\n",
    "        y_idx = int((point[1] - min_bound[1]) // grid_size_y)\n",
    "\n",
    "        # Ensure the point is within the grid boundaries\n",
    "        x_idx = min(grid_size[0] - 1, max(0, x_idx))\n",
    "        y_idx = min(grid_size[1] - 1, max(0, y_idx))\n",
    "\n",
    "        # Add the point to the corresponding grid cell\n",
    "        grid_cells[x_idx][y_idx].append(point)\n",
    "\n",
    "    return grid_cells\n",
    "\n",
    "# Example usage:\n",
    "# Replace \"path/to/your_point_cloud.obj\" with the actual path to your .obj file\n",
    "# point_cloud = o3d.io.read_point_cloud(\"path/to/your_point_cloud.obj\")\n",
    "# if point_cloud is not None:\n",
    "#     point_cloud_array = np.asarray(point_cloud.points)\n",
    "#     grid_cells = divide_point_cloud_into_grid(point_cloud_array)\n",
    "\n",
    "def calculate_minimum_in_grid_cells(grid_cells, grid_division=45):\n",
    "    # Calculate the minimum value in each grid cell\n",
    "    min_points = []\n",
    "    for row in grid_cells:\n",
    "        for cell in row:\n",
    "            if cell:\n",
    "                # Get the index of the point with the minimum z-coordinate value\n",
    "                min_z_index = np.argmin([point[2] for point in cell])\n",
    "                min_point = cell[min_z_index]\n",
    "                min_points.append(min_point)\n",
    "            else:\n",
    "                # If a grid cell is empty, append a placeholder value (e.g., None)\n",
    "                min_points.append(None)\n",
    "\n",
    "    return min_points[:grid_division*grid_division]  # Return the first 1024 points (in case there are more than 1024 cells)\n",
    "\n",
    "# Example usage:\n",
    "# Assuming you have already imported and divided the point cloud into grid_cells\n",
    "# min_values = calculate_minimum_in_grid_cells(grid_cells)\n",
    "\n",
    "# MAXIMUM IS FOR CALCULATING THE HEIGHT OF THE RDLA LICHEN DISTANCE AWAY FROM THE SAMPLE MESH\n",
    "# but we don't use it because we opt for calculating distance from samples mesh by point to mesh calculation\n",
    "# using cloud compare\n",
    "def calculate_maximum_in_grid_cells(grid_cells, grid_division=45):\n",
    "    # Calculate the maximum value in each grid cell\n",
    "    max_points = []\n",
    "    for row in grid_cells:\n",
    "        for cell in row:\n",
    "            if cell:\n",
    "                # Get the index of the point with the maximum z-coordinate value\n",
    "                max_z_index = np.argmax([point[2] for point in cell])\n",
    "                max_point = cell[max_z_index]\n",
    "                max_points.append(max_point)\n",
    "            else:\n",
    "                # If a grid cell is empty, append a placeholder value (e.g., None)\n",
    "                max_points.append(None)\n",
    "\n",
    "    return max_points[:grid_division*grid_division]  # Return the first 1024 points (in case there are more than 1024 cells)\n",
    "\n",
    "# Example usage:\n",
    "# Assuming you have already imported and divided the point cloud into grid_cells\n",
    "# max_values = calculate_maximum_in_grid_cells(grid_cells)\n",
    "\n",
    "def save_points_to_ply_file(min_points, file_path):\n",
    "    # Filter out the None elements from min_points\n",
    "    valid_points = [point for point in min_points if point is not None]\n",
    "\n",
    "    # Convert the list of points to a numpy array\n",
    "    point_array = np.array(valid_points)\n",
    "\n",
    "    # Create an Open3D point cloud\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(point_array)\n",
    "\n",
    "    # Save the point cloud to a .ply file\n",
    "    o3d.io.write_point_cloud(file_path, pcd)\n",
    "\n",
    "# ENSURES SAMPLING HISTOGRAM FOLLOWS THAT OF THE LICHEN LASER SCAN \n",
    "def histogram_correction(mesh, dust_accumulation_file_name, file_path):\n",
    "    # create a scene, put the seed mesh into the scene\n",
    "    scene = o3d.t.geometry.RaycastingScene()\n",
    "    _ = scene.add_triangles(mesh)\n",
    "    \n",
    "    # Resample dust accumulation points according to the laser scan distribution\n",
    "    # get dust accumulation points\n",
    "    query_points = np.float32(import_point_cloud_from_obj(dust_accumulation_file_name))\n",
    "    \n",
    "    #calculate distance from dust to seed mesh\n",
    "    distance_dust = scene.compute_distance(query_points)\n",
    "    distance_dust = distance_dust.numpy()  # Extract distances as a NumPy array\n",
    "    \n",
    "    # Sort points based on distances\n",
    "    sorted_indices = np.argsort(distance_dust)  # Indices of points sorted by distance\n",
    "    closest_points = query_points[sorted_indices]  # Points sorted by closest to farthest\n",
    "    \n",
    "    # define the bin widths in metre\n",
    "    bins_mm = np.array([0.0, 0.00125, 0.00250, 0.00375, 0.00500, 0.00625, 0.00750, 0.00875, 0.01000, 0.01125,\n",
    "                0.01250, 0.01375, 0.01500, 0.01625, 0.01750, 0.01875, 0.02000, 0.02125, 0.02250, 0.02375, 0.02500,\n",
    "                0.02625, 0.02750, 0.02875, 0.03000, 0.03125, 0.03250, 0.03375, 0.03500, 0.03625, 0.03750])\n",
    "    \n",
    "    # calculate how many points fall in each bin \n",
    "    hist, bins = np.histogram(distance_dust, bins=bins_mm)\n",
    "    \n",
    "    # in case there are slices with 0 points\n",
    "    for i, j in enumerate(hist):\n",
    "        if hist[i] == 0:\n",
    "            hist[i] = 1\n",
    "    \n",
    "    # calculate the area of the seed mesh and convert to number of points for sampling\n",
    "    # 10 times more points than the RDLA from the c++ code\n",
    "    # surface area to point number conversion\n",
    "    mesh = o3d.io.read_triangle_mesh(os.path.join(seed_folder,\"1024_F602.obj\"))\n",
    "    surface_area = mesh.get_surface_area()\n",
    "    dust_population = (surface_area / (pow(0.01, 2) * np.pi)) * 2000\n",
    "    \n",
    "    # collect the points (as coordinates) into 2D array: bin_array\n",
    "    lower_bin_boundary = 0\n",
    "    bin_array = []\n",
    "    for i in hist:\n",
    "        bin_array.append(closest_points[lower_bin_boundary:(lower_bin_boundary + i)])\n",
    "        lower_bin_boundary = lower_bin_boundary + i\n",
    "    \n",
    "    # collect the distance of laser scan lichen points from the stone 30 surface \n",
    "    mean_dist_array_lichen = []\n",
    "    with open(r'***********************' + '.csv', newline='') as csvfile2:\n",
    "        reader2 = csv.reader(csvfile2, delimiter=' ')\n",
    "        for row in reader2:\n",
    "            mean_dist_array_lichen.append(float(row[0]))\n",
    "     \n",
    "    # bin the laser scan lichen points, then calculate probability of these points of falling into the bins/bands\n",
    "    hist_l, bins_l = np.histogram(mean_dist_array_lichen, bins=bins_mm, density=True)\n",
    "    hist_lichen = hist_l*0.00125 # 0.00125 being the width of the bins in metre\n",
    "    hist_lichen = dust_population * hist_lichen\n",
    "    \n",
    "    counter = 1\n",
    "    new_dust_points = []\n",
    "    \n",
    "    # sample from bin_array which has coordinates of points in 2-D array\n",
    "    # using accumulating hist_lichen/hist\n",
    "    previous = 0\n",
    "    for index1, value1 in enumerate(bin_array):\n",
    "        counter = 1\n",
    "        previous = 0\n",
    "        for index2, value2 in enumerate(bin_array[index1]):\n",
    "            min_bound = (hist_lichen[index1]/hist[index1]) * counter\n",
    "            counter = counter + 1\n",
    "            round_down = np.floor(min_bound)\n",
    "            if round_down > (previous+0.5):\n",
    "                new_dust_points.append(bin_array[index1][index2])\n",
    "                previous = round_down\n",
    "    \n",
    "    # Convert the array to an Open3D point cloud\n",
    "    pcd_adjust = o3d.geometry.PointCloud()\n",
    "    pcd_adjust.points = o3d.utility.Vector3dVector(new_dust_points)\n",
    "    \n",
    "    # Save as a .ply file\n",
    "    o3d.io.write_point_cloud(file_path, pcd_adjust)\n",
    "\n",
    "\n",
    "\n",
    "#f = open('readme.txt', 'w') \n",
    "# Loop through all .obj files in the input folder\n",
    "for file in sorted(Path(input_folder).iterdir(), key=os.path.getmtime):\n",
    "    if file.parts[-1].endswith('.csv'): # .endswith(\".csv\"):\n",
    "\n",
    "        #################################################################\n",
    "        ### DISTANCE MAPPING\n",
    "        \n",
    "        # import point cloud (uses open3d)\n",
    "        #point_cloud = import_point_cloud_from_obj(input_folder + '\\\\' + file)\n",
    "        np_ptcld = np.genfromtxt(input_folder + '\\\\' + file.parts[-1], delimiter=',')\n",
    "        pcd = o3d.geometry.PointCloud()\n",
    "        pcd.points = o3d.utility.Vector3dVector(np_ptcld)\n",
    "        \n",
    "        # Get the minimum and maximum coordinates of the point cloud\n",
    "        min_bound = np.asarray(pcd.get_min_bound())\n",
    "        max_bound = np.asarray(pcd.get_max_bound())\n",
    "        \n",
    "        # Adjust the minimum and maximum bounds to create the bounding cube with clearance\n",
    "        min_bound -= clearance\n",
    "        max_bound += clearance\n",
    "        \n",
    "        # Calculate the number of points needed in each dimension of the grid\n",
    "        num_points_x = int(np.ceil((max_bound[0] - min_bound[0]) / distanceMapGridstep))\n",
    "        num_points_y = int(np.ceil((max_bound[1] - min_bound[1]) / distanceMapGridstep))\n",
    "        num_points_z = int(np.ceil((max_bound[2] - min_bound[2]) / distanceMapGridstep))\n",
    "        \n",
    "        # Generate the query points grid using numpy.mgrid with the calculated number of points\n",
    "        grid_x, grid_y, grid_z = np.mgrid[min_bound[0]:max_bound[0]:num_points_x*1j,\n",
    "                                          min_bound[1]:max_bound[1]:num_points_y*1j,\n",
    "                                          min_bound[2]:max_bound[2]:num_points_z*1j]\n",
    "        \n",
    "        # Combine the grid points into a single array of shape [num_points, 3]\n",
    "        query_points = np.column_stack((grid_x.ravel(), grid_y.ravel(), grid_z.ravel())).astype(np.float32)\n",
    "\n",
    "        # Convert the array to a PointCloud object in Open3D\n",
    "        query_point_cloud = o3d.geometry.PointCloud()\n",
    "        query_point_cloud.points = o3d.utility.Vector3dVector(query_points.reshape(-1, 3))\n",
    "        \n",
    "        # Calculate distances from the point cloud to the query points\n",
    "        distances = o3d.geometry.PointCloud.compute_point_cloud_distance(query_point_cloud, pcd)\n",
    "        \n",
    "        # Convert distances to a NumPy array for reshaping and visualization\n",
    "        distances_np = np.asarray(distances)\n",
    "        \n",
    "        # Reshape the distances_np array to match the shape of the query grid\n",
    "        query_shape = (len(grid_x), len(grid_x[0]), len(grid_x[0][0]))\n",
    "        distances_reshaped = distances_np.reshape(query_shape)\n",
    "        \n",
    "        # Define the set distance you want to find points at (e.g., 0.875 mm) for\n",
    "        # 1.75 mm branch thickness. The 0.003 buffer zone around the shell of wanted\n",
    "        # distance map values ensures enough points are obtained for the mesh reconstruction.\n",
    "        set_distance_min = 0.000875 - 0.0003\n",
    "        set_distance_max = 0.000875 + 0.0003\n",
    "        #set_distance_min = 0\n",
    "        #set_distance_max = 10\n",
    "        \n",
    "        # Find the indices of the query points that are approximately 1.5 mm away from the point cloud\n",
    "        indices_dist_range = np.asarray(np.logical_and(np.abs(distances_np) >= set_distance_min,\n",
    "                                               np.abs(distances_np) <= set_distance_max)).nonzero()\n",
    "        \n",
    "        # Retrieve the coordinates of the points that are approximately 1.5 mm away from the point cloud\n",
    "        points_dist_range = np.asarray(query_point_cloud.points)[indices_dist_range]\n",
    "\n",
    "        #save_points_to_ply_file(points_dist_range, output_folder + '\\\\' + file[5:9] + 'RDLA_' + strftime('%Y-%m-%d_%H-%M', localtime(start_time)) \n",
    "        #                        + 'step' + str(distanceMapGridstep)[4:] + 'min' + str(set_distance_min)[4:]\n",
    "        #                        + 'max' + str(set_distance_max)[4:] + '.ply')\n",
    "\n",
    "        #################################################################\n",
    "        ### BALL PIVOTING\n",
    "        # create meshlab object\n",
    "        ball_pivot_mesh = ml.MeshSet()\n",
    "\n",
    "        # fill meshlab object with points from the extracted distance map\n",
    "        vertices = np.asarray(points_dist_range)\n",
    "        mesh_converted = ml.Mesh(vertices)\n",
    "        ball_pivot_mesh.add_mesh(mesh_converted, \"ball_pivoted_mesh\")\n",
    "\n",
    "        # if point cloud simplification is needed\n",
    "        #ball_pivot_mesh.apply_filter(\"generate_simplified_point_cloud\", radius = ml.AbsoluteValue(0.0008))\n",
    "        \n",
    "        # Apply the ball pivot algorithm for meshing distance map points\n",
    "        ball_pivot_mesh.apply_filter(\"generate_surface_reconstruction_ball_pivoting\", ballradius = ml.AbsoluteValue(0.0009), creasethr = 150)\n",
    "\n",
    "        # close any remaining holes from the ball-pivoting\n",
    "        ball_pivot_mesh.apply_filter(\"meshing_close_holes\")\n",
    "\n",
    "        # file name for ball pivot mesh output\n",
    "        ball_pivot_file_name = os.path.join(output_folder, file.parts[-1][5:9] + 'RDLA_ball_recon' + strftime('%Y-%m-%d_%H-%M', localtime(start_time)) \n",
    "                                + 'step' + str(distanceMapGridstep)[4:] + 'min' + str(set_distance_min)[4:]\n",
    "                                + 'max' + str(set_distance_max)[4:] + '.ply')\n",
    "        \n",
    "        # save mesh using pymeshlab saving method\n",
    "        ball_pivot_mesh.save_current_mesh(ball_pivot_file_name)\n",
    "\n",
    "        #################################################################\n",
    "        # DUST ACCUMULATION\n",
    "\n",
    "        # Apply the generate_dust_accumulation_point_cloud function\n",
    "        ball_pivot_mesh.apply_filter(\"generate_dust_accumulation_point_cloud\", dust_dir = [0,-0.70711,0.70711], nparticles = 3)\n",
    "\n",
    "        dust_accumulation_file_name = os.path.join(output_folder, file.parts[-1][5:9] + 'RDLA_dust' + strftime('%Y-%m-%d_%H-%M', localtime(start_time)) \n",
    "                                + 'step' + str(distanceMapGridstep)[4:] + 'min' + str(set_distance_min)[4:]\n",
    "                                + 'max' + str(set_distance_max)[4:] + '.ply')\n",
    "        \n",
    "        # save mesh using pymeshlab saving method\n",
    "        ball_pivot_mesh.save_current_mesh(dust_accumulation_file_name)\n",
    "\n",
    "        # DISTANCE BETWEEN DUST AND SEED MESH\n",
    "        pcd = o3d.io.read_point_cloud(dust_accumulation_file_name, format='ply')\n",
    "        mesh = o3d.io.read_triangle_mesh(os.path.join(seed_folder,\"1024_F602.obj\"))\n",
    "        mesh = o3d.t.geometry.TriangleMesh.from_legacy(mesh)\n",
    "\n",
    "        # perform histogram correction to ensure the dust accumulation (sampling)\n",
    "        # follows the distribution of the laser scanned lichen data\n",
    "        dust_accumulation_corrected_file_name = os.path.join(output_folder, file.parts[-1][5:9] + 'RDLA_dust' + strftime('%Y-%m-%d_%H-%M', localtime(start_time)) \n",
    "                        + 'step' + str(distanceMapGridstep)[4:] + 'min' + str(set_distance_min)[4:]\n",
    "                        + 'max' + str(set_distance_max)[4:] + '_corrected.ply')\n",
    "        \n",
    "        histogram_correction(mesh, dust_accumulation_file_name, dust_accumulation_corrected_file_name)\n",
    "        \n",
    "        #################################################################\n",
    "        ### MIN Z GRID\n",
    "        point_cloud = import_point_cloud_from_obj(dust_accumulation_file_name)\n",
    "        \n",
    "        grid_division = 50\n",
    "        divided_ptcld = divide_point_cloud_into_grid(point_cloud, (grid_division,grid_division))\n",
    "        final_cell = calculate_minimum_in_grid_cells(divided_ptcld, grid_division)\n",
    "        #print(len(final_cell))\n",
    "\n",
    "        min_z_file_name = os.path.join(output_folder, filepart + 'RDLA_lowest_z' + strftime('%Y-%m-%d_%H-%M', localtime(start_time)) \n",
    "                                + 'step' + str(distanceMapGridstep)[4:] + 'min' + str(set_distance_min)[4:]\n",
    "                                + 'max' + str(set_distance_max)[4:] + '.ply')\n",
    "        \n",
    "        save_points_to_ply_file(final_cell, min_z_file_name)\n",
    "        \n",
    "        #################################################################\n",
    "        ### NORMALS CALCULATION\n",
    "\n",
    "        # create meshlab object\n",
    "        output_mesh = ml.MeshSet()\n",
    "\n",
    "        # Filter out the None elements from min_points\n",
    "        valid_points = [point for point in final_cell if point is not None]\n",
    "        # Convert the list of points to a numpy array\n",
    "        point_array = np.array(valid_points)\n",
    "                \n",
    "        # fill meshlab object with points from the extracted distance map\n",
    "        vertices = np.asarray(point_array)\n",
    "        mesh_converted = ml.Mesh(vertices)\n",
    "        output_mesh.add_mesh(mesh_converted, \"ball_pivoted_mesh\")\n",
    "        \n",
    "        output_mesh.apply_filter(\"compute_normal_for_point_clouds\", k = 6, flipflag = True, viewpos = [0, 0, 1])\n",
    "        #################################################################\n",
    "        ### SURFACE RECONSTUCTION SCREENED POISSON\n",
    "\n",
    "        output_mesh.apply_filter(\"generate_surface_reconstruction_screened_poisson\", scale = 1.0)\n",
    "        \n",
    "        #################################################################\n",
    "        ### SIMPLIFICATION: QUADRATIC EDGE COLLAPSE DECIMATION: 1024\n",
    "        output_mesh.apply_filter(\"meshing_decimation_quadric_edge_collapse\", targetfacenum = 1024, qualitythr = 1.0, \n",
    "                                 preservenormal = True, preservetopology = True)\n",
    "        \n",
    "        final_1024_mesh = os.path.join(output_folder, filepart + 'RDLA_final_1024' + strftime('%Y-%m-%d_%H-%M', localtime(start_time)) \n",
    "                                + 'step' + str(distanceMapGridstep)[4:] + 'min' + str(set_distance_min)[4:]\n",
    "                                + 'max' + str(set_distance_max)[4:] + '.ply')\n",
    "        \n",
    "        # save mesh using pymeshlab saving method\n",
    "        output_mesh.save_current_mesh(final_1024_mesh)\n",
    "\n",
    "        \n",
    "        for file2 in os.listdir(seed_folder):\n",
    "            if file2.endswith(\"1024_F602.obj\"):\n",
    "                cli = cc.CloudCompareCLI()\n",
    "                cmd = cli.new_command()\n",
    "                cmd.silent()  # Disable console\n",
    "                cmd.open(seed_folder + '\\\\' + file2)  # Read file\n",
    "                cmd.open(final_1024_mesh)  # Read file\n",
    "                cmd.c2m_dist()\n",
    "                #cmd.cloud_export_format(cc.CLOUD_EXPORT_FORMAT.ASCII, extension=\"csv\")\n",
    "                #cmd.save_clouds('GAMMA_CALCULATE_OUTPUT\\\\' + file.parts[-1][:27] + '.csv')\n",
    "                print(cmd)\n",
    "                cmd.execute()\n",
    "\n",
    "# End timing\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "print(f\"Execution time: {execution_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0249f8f0-c3df-4d84-ac19-78ebc31d43a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################\n",
    "### HISTOGRAM COMPARISON\n",
    "# create a scene, put the seed mesh into the scene\n",
    "scene2 = o3d.t.geometry.RaycastingScene()\n",
    "_ = scene2.add_triangles(mesh)\n",
    "\n",
    "#calculate distance from dust to seed mesh\n",
    "distance_dust_adjust = scene2.compute_distance(new_dust_points).numpy()\n",
    "#print(distance_dust_adjust)\n",
    "hist3, bins3 = np.histogram(distance_dust_adjust, bins=bins_mm)\n",
    "\n",
    "distance_dust_adjust = distance_dust_adjust*1000\n",
    "mean_dist_array_lichen = np.array(mean_dist_array_lichen)*1000\n",
    "\n",
    "# Define histograms for the sampling and laser scanning\n",
    "hist_adjust_dust, _ = np.histogram(distance_dust_adjust, bins = bins_mm*1000, density = True)\n",
    "hist_lichen, _ = np.histogram(mean_dist_array_lichen, bins = bins_mm*1000, density = True)\n",
    "\n",
    "# calculate the expected dust accumulation (f_exp) for the chi-squared test \n",
    "# removes the last two bins due to aberrant empty bins. \n",
    "expected_dust = len(distance_dust_adjust) * hist_lichen[:-2] * (1/sum(hist_lichen[:-2])) # latter term is ~1.25 and adjusts for \n",
    "                                                                                    # I believe the bin width being 80% of 1 cm\n",
    "\n",
    "# chi squared result comparing the dust accumulation to the laser scan distribution of Ramalina siliquosa\n",
    "chisquare(f_obs = hist3[:-2], f_exp = expected_dust, ddof=0, axis=0)\n",
    "\n",
    "# now plot the laser scan lichen distribution as a probability density histogram along with\n",
    "# the dust accumulation\n",
    "\n",
    "fig = plt.figure(figsize=(16, 7)) \n",
    "gs1 = plt.subplot2grid((1, 2), (0, 0))\n",
    "ax = [fig.add_subplot(1,2,i+1) for i in range(2)]\n",
    "\n",
    "for a in ax:\n",
    "    a.set_xticklabels([])\n",
    "    a.set_yticklabels([])\n",
    "    a.set_aspect('equal')\n",
    "\n",
    "fig.subplots_adjust(wspace=0.05, hspace=0.05)\n",
    "\n",
    "ax1 = plt.subplot2grid((1, 2), (0, 0))\n",
    "ax2 = plt.subplot2grid((1, 2), (0, 1))\n",
    "\n",
    "ax1.hist(abs(np.asarray(distance_dust_adjust)), density = True, color= [(0, 114/255.0, 178/255.0)], edgecolor='black', linewidth=1.2, bins=bins_mm*1000, label = 'Dust accumulation per point distance from F602 seed mesh')\n",
    "ax2.hist(abs(np.asarray(mean_dist_array_lichen)), density = True, color= [(0, 158/255.0, 115/255.0)], edgecolor='black', linewidth=1.2, bins=bins_mm*1000, label = 'Laser scan per point distance of Ramalina siliquosa from rock surface')\n",
    "\n",
    "textfontsize = 20\n",
    "ticksize = 16\n",
    "    \n",
    "ax1.set_ylabel('Probability density', fontsize=textfontsize)\n",
    "ax1.set_xlabel('Distance (mm)', fontsize=textfontsize)\n",
    "ax2.set_xlabel('Distance (mm)', fontsize=textfontsize)\n",
    "\n",
    "ax1.tick_params(axis='both', which='major', labelsize=ticksize)\n",
    "ax2.tick_params(axis='both', which='major', labelsize=ticksize)\n",
    "\n",
    "ax1.tick_params(direction='out', length=6, width=1.5, colors='k',\n",
    "               grid_color='k', grid_alpha=1.0)\n",
    "ax2.tick_params(direction='out', length=6, width=1.5, colors='k',\n",
    "               grid_color='k', grid_alpha=1.0)\n",
    "\n",
    "ax2.yaxis.set_tick_params(labelleft=False)\n",
    "\n",
    "lines_array = []\n",
    "labels_array = []\n",
    "  \n",
    "lines, labels = ax1.get_legend_handles_labels()\n",
    "lines_array.extend(lines)\n",
    "labels_array.extend(labels)\n",
    "lines, labels = ax2.get_legend_handles_labels()\n",
    "lines_array.extend(lines)\n",
    "labels_array.extend(labels)\n",
    "\n",
    "fig.legend(lines_array, labels_array, bbox_to_anchor=(0.675,0.97), loc=\"upper right\",\n",
    "                          bbox_transform=plt.gcf().transFigure, fontsize=textfontsize)\n",
    "plt.subplots_adjust(left=0.0, bottom=0.1, top=0.8)\n",
    "plt.savefig(r'***********************.png', \n",
    "            dpi= 1000, bbox_inches = 'tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "open3d_env",
   "language": "python",
   "name": "open3d_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
